"""
Data models for AI code detection.

This module defines the core data structures used throughout the detection system,
including confidence levels, analysis results, and project summaries.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import List, Dict, Optional, Any
from datetime import datetime


class ConfidenceLevel(Enum):
    """Confidence levels for AI-generated code detection."""
    
    NONE = "none"  # 0-20%: Likely human-written
    LOW = "low"  # 20-40%: Minor AI patterns
    MEDIUM = "medium"  # 40-60%: Moderate AI influence
    HIGH = "high"  # 60-80%: Strong AI patterns
    VERY_HIGH = "very_high"  # 80-100%: Almost certainly AI-generated
    
    @classmethod
    def from_score(cls, score: float) -> "ConfidenceLevel":
        """Convert numeric score (0.0-1.0) to confidence level."""
        if score < 0.2:
            return cls.NONE
        elif score < 0.4:
            return cls.LOW
        elif score < 0.6:
            return cls.MEDIUM
        elif score < 0.8:
            return cls.HIGH
        else:
            return cls.VERY_HIGH
    
    def to_score_range(self) -> tuple[float, float]:
        """Get the score range for this confidence level."""
        ranges = {
            self.NONE: (0.0, 0.2),
            self.LOW: (0.2, 0.4),
            self.MEDIUM: (0.4, 0.6),
            self.HIGH: (0.6, 0.8),
            self.VERY_HIGH: (0.8, 1.0)
        }
        return ranges[self]


@dataclass
class DetectionPattern:
    """A single detected AI pattern."""
    
    pattern_type: str
    description: str
    line_number: int
    code_snippet: str
    confidence: float
    severity: str = "medium"  # low, medium, high, critical
    remediation_suggestion: Optional[str] = None
    
    def __str__(self) -> str:
        return f"{self.pattern_type} at line {self.line_number} ({self.confidence:.2%})"


@dataclass
class FileAnalysis:
    """Analysis results for a single file."""
    
    file_path: str
    language: str
    lines_of_code: int
    confidence_score: float
    confidence_level: ConfidenceLevel
    patterns: List[DetectionPattern] = field(default_factory=list)
    analysis_phases: Dict[str, Any] = field(default_factory=dict)
    scan_duration_ms: float = 0.0
    error: Optional[str] = None
    
    @property
    def is_ai_generated(self) -> bool:
        """Check if file is likely AI-generated (>40% confidence)."""
        return self.confidence_score >= 0.4
    
    @property
    def pattern_count(self) -> int:
        """Total number of patterns detected."""
        return len(self.patterns)
    
    @property
    def high_confidence_patterns(self) -> List[DetectionPattern]:
        """Get patterns with >70% confidence."""
        return [p for p in self.patterns if p.confidence >= 0.7]
    
    def __str__(self) -> str:
        return (
            f"{self.file_path}: {self.confidence_level.value} "
            f"({self.confidence_score:.2%}) - {self.pattern_count} patterns"
        )


@dataclass
class DetectionResult:
    """Complete detection results for a scan operation."""
    
    scan_id: str
    timestamp: datetime
    target_path: str
    total_files: int
    scanned_files: int
    skipped_files: int
    file_analyses: List[FileAnalysis] = field(default_factory=list)
    overall_confidence: float = 0.0
    scan_duration_seconds: float = 0.0
    configuration: Dict[str, Any] = field(default_factory=dict)
    errors: List[str] = field(default_factory=list)
    
    @property
    def ai_generated_files(self) -> List[FileAnalysis]:
        """Get files likely generated by AI (>40% confidence)."""
        return [f for f in self.file_analyses if f.is_ai_generated]
    
    @property
    def ai_file_count(self) -> int:
        """Number of AI-generated files."""
        return len(self.ai_generated_files)
    
    @property
    def ai_percentage(self) -> float:
        """Percentage of files that are AI-generated."""
        if self.scanned_files == 0:
            return 0.0
        return (self.ai_file_count / self.scanned_files) * 100
    
    @property
    def total_patterns(self) -> int:
        """Total number of patterns detected across all files."""
        return sum(f.pattern_count for f in self.file_analyses)
    
    def get_summary(self) -> Dict[str, Any]:
        """Get a summary of detection results."""
        return {
            "scan_id": self.scan_id,
            "timestamp": self.timestamp.isoformat(),
            "target": self.target_path,
            "files": {
                "total": self.total_files,
                "scanned": self.scanned_files,
                "skipped": self.skipped_files,
                "ai_generated": self.ai_file_count,
                "ai_percentage": round(self.ai_percentage, 2)
            },
            "confidence": {
                "overall": round(self.overall_confidence, 4),
                "level": ConfidenceLevel.from_score(self.overall_confidence).value
            },
            "patterns": {
                "total": self.total_patterns,
                "average_per_file": round(self.total_patterns / max(self.scanned_files, 1), 2)
            },
            "performance": {
                "duration_seconds": round(self.scan_duration_seconds, 2),
                "files_per_second": round(self.scanned_files / max(self.scan_duration_seconds, 0.1), 2)
            },
            "errors": len(self.errors)
        }
    
    def __str__(self) -> str:
        return (
            f"Scan {self.scan_id}: {self.scanned_files} files, "
            f"{self.ai_file_count} AI-generated ({self.ai_percentage:.1f}%), "
            f"Overall confidence: {self.overall_confidence:.2%}"
        )


@dataclass
class ProjectAnalysis:
    """High-level project analysis and statistics."""
    
    project_path: str
    detection_result: DetectionResult
    quality_gate_passed: bool = True
    quality_gate_failures: List[str] = field(default_factory=list)
    remediation_suggestions: List[Dict[str, Any]] = field(default_factory=list)
    compliance_report: Optional[Dict[str, Any]] = None
    
    @property
    def is_acceptable(self) -> bool:
        """Check if project meets quality standards."""
        return self.quality_gate_passed
    
    @property
    def risk_level(self) -> str:
        """Assess overall project risk level."""
        confidence = self.detection_result.overall_confidence
        if confidence < 0.3:
            return "low"
        elif confidence < 0.5:
            return "medium"
        elif confidence < 0.7:
            return "high"
        else:
            return "critical"
    
    def get_executive_summary(self) -> Dict[str, Any]:
        """Get executive summary for reporting."""
        return {
            "project": self.project_path,
            "risk_level": self.risk_level,
            "quality_gate": "PASSED" if self.quality_gate_passed else "FAILED",
            "statistics": self.detection_result.get_summary(),
            "failures": self.quality_gate_failures,
            "recommendations": len(self.remediation_suggestions)
        }
    
    def __str__(self) -> str:
        status = "✓ PASSED" if self.quality_gate_passed else "✗ FAILED"
        return f"Project Analysis [{status}]: {self.project_path} - Risk: {self.risk_level}"


@dataclass
class ScanConfiguration:
    """Configuration for scan operations."""
    
    # File filtering
    include_extensions: List[str] = field(default_factory=lambda: [
        ".py", ".js", ".ts", ".java", ".cpp", ".c", ".cs", ".rb", ".go", ".rs"
    ])
    exclude_patterns: List[str] = field(default_factory=lambda: [
        "node_modules/*", "venv/*", "*.min.js", "__pycache__/*", ".git/*"
    ])
    max_file_size_kb: int = 1024  # 1MB default
    
    # Detection thresholds
    confidence_threshold: float = 0.4  # Report files above this
    pattern_threshold: int = 3  # Minimum patterns to flag
    
    # Quality gates
    max_ai_confidence: float = 0.6  # Block if exceeded
    max_ai_file_percentage: float = 30.0  # Block if exceeded
    
    # Performance
    max_workers: int = 4  # Parallel processing
    timeout_seconds: int = 300  # 5 minutes
    
    # Output
    output_format: str = "json"  # json, yaml, html, pdf
    verbose: bool = False
    
    # Enterprise features
    enable_remediation: bool = False
    enable_quality_gates: bool = False
    enable_compliance_report: bool = False
    
    def validate(self) -> List[str]:
        """Validate configuration and return any errors."""
        errors = []
        
        if not 0 <= self.confidence_threshold <= 1:
            errors.append("confidence_threshold must be between 0 and 1")
        
        if not 0 <= self.max_ai_confidence <= 1:
            errors.append("max_ai_confidence must be between 0 and 1")
        
        if not 0 <= self.max_ai_file_percentage <= 100:
            errors.append("max_ai_file_percentage must be between 0 and 100")
        
        if self.max_workers < 1:
            errors.append("max_workers must be at least 1")
        
        if self.output_format not in ["json", "yaml", "html", "pdf", "text"]:
            errors.append(f"Invalid output_format: {self.output_format}")
        
        return errors
